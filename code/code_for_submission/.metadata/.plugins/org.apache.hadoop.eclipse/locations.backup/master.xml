<?xml version="1.0" encoding="UTF-8" standalone="no"?><configuration>
<property><name>mapreduce.job.ubertask.enable</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.client.submit.file.replication</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.cleaner.interval-ms</name><value>86400000</value><source>programatically</source></property>
<property><name>dfs.image.transfer.bandwidthPerSec</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.done-dir</name><value>${yarn.app.mapreduce.am.staging-dir}/history/done</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.healthchecker.interval</name><value>60000</value><source>programatically</source></property>
<property><name>eclipse.plug-in.jobtracker.port</name><value>50020</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.staging.root.dir</name><value>${hadoop.tmp.dir}/mapred/staging</value><source>programatically</source></property>
<property><name>dfs.block.access.token.lifetime</name><value>600</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.file.impl</name><value>org.apache.hadoop.fs.local.LocalFs</value><source>programatically</source></property>
<property><name>mapreduce.client.completion.pollinterval</name><value>5000</value><source>programatically</source></property>
<property><name>mapreduce.job.ubertask.maxreduces</name><value>1</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.client.thread-count</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.memory.limit.percent</name><value>0.25</value><source>programatically</source></property>
<property><name>hadoop.ssl.keystores.factory.class</name><value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.kerberos.keytab</name><value>${user.home}/hadoop.keytab</value><source>programatically</source></property>
<property><name>io.seqfile.sorter.recordlimit</name><value>1000000</value><source>programatically</source></property>
<property><name>s3.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>mapreduce.task.io.sort.factor</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.job.speculative.speculativecap</name><value>0.1</value><source>programatically</source></property>
<property><name>dfs.namenode.num.checkpoints.retained</name><value>2</value><source>programatically</source></property>
<property><name>dfs.namenode.delegation.token.renew-interval</name><value>86400000</value><source>programatically</source></property>
<property><name>io.map.index.interval</name><value>128</value><source>programatically</source></property>
<property><name>nfs3.mountd.port</name><value>4242</value><source>programatically</source></property>
<property><name>s3.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>mapreduce.task.files.preserve.failedtasks</name><value>false</value><source>programatically</source></property>
<property><name>dfs.namenode.http-address</name><value>0.0.0.0:50070</value><source>programatically</source></property>
<property><name>ha.zookeeper.session-timeout.ms</name><value>5000</value><source>programatically</source></property>
<property><name>hadoop.hdfs.configuration.version</name><value>1</value><source>programatically</source></property>
<property><name>eclipse.plug-in.masters.colocate</name><value>yes</value><source>programatically</source></property>
<property><name>s3.replication</name><value>3</value><source>programatically</source></property>
<property><name>dfs.datanode.balance.bandwidthPerSec</name><value>1048576</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.connect.timeout</name><value>180000</value><source>programatically</source></property>
<property><name>dfs.journalnode.rpc-address</name><value>0.0.0.0:8485</value><source>programatically</source></property>
<property><name>hadoop.ssl.enabled</name><value>false</value><source>programatically</source></property>
<property><name>eclipse.plug-in.location.name</name><value>master</value><source>programatically</source></property>
<property><name>mapreduce.job.counters.max</name><value>120</value><source>programatically</source></property>
<property><name>dfs.datanode.readahead.bytes</name><value>4193404</value><source>programatically</source></property>
<property><name>ipc.client.connect.max.retries.on.timeouts</name><value>45</value><source>programatically</source></property>
<property><name>mapreduce.job.complete.cancel.delegation.tokens</name><value>true</value><source>programatically</source></property>
<property><name>dfs.client.failover.max.attempts</name><value>15</value><source>programatically</source></property>
<property><name>dfs.namenode.checkpoint.dir</name><value>file://${hadoop.tmp.dir}/dfs/namesecondary</value><source>programatically</source></property>
<property><name>dfs.namenode.replication.work.multiplier.per.iteration</name><value>2</value><source>programatically</source></property>
<property><name>fs.trash.interval</name><value>0</value><source>programatically</source></property>
<property><name>ha.health-monitor.check-interval.ms</name><value>1000</value><source>programatically</source></property>
<property><name>hadoop.jetty.logs.serve.aliases</name><value>true</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.kerberos.principal</name><value>HTTP/_HOST@LOCALHOST</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.taskmemorymanager.monitoringinterval</name><value>5000</value><source>programatically</source></property>
<property><name>mapreduce.job.reduce.shuffle.consumer.plugin.class</name><value>org.apache.hadoop.mapreduce.task.reduce.Shuffle</value><source>programatically</source></property>
<property><name>s3native.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>dfs.namenode.edits.dir</name><value>${dfs.namenode.name.dir}</value><source>programatically</source></property>
<property><name>ha.health-monitor.sleep-after-disconnect.ms</name><value>1000</value><source>programatically</source></property>
<property><name>dfs.encrypt.data.transfer</name><value>false</value><source>programatically</source></property>
<property><name>dfs.datanode.http.address</name><value>0.0.0.0:50075</value><source>programatically</source></property>
<property><name>dfs.namenode.write.stale.datanode.ratio</name><value>0.5f</value><source>programatically</source></property>
<property><name>dfs.client.use.datanode.hostname</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.jobhistory.task.numberprogresssplits</name><value>12</value><source>programatically</source></property>
<property><name>mapreduce.map.cpu.vcores</name><value>1</value><source>programatically</source></property>
<property><name>hadoop.security.instrumentation.requires.admin</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.security.authorization</name><value>false</value><source>programatically</source></property>
<property><name>dfs.namenode.fs-limits.min-block-size</name><value>1048576</value><source>programatically</source></property>
<property><name>dfs.client.failover.connection.retries.on.timeouts</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.filter.group</name><value>(objectClass=group)</value><source>programatically</source></property>
<property><name>mapreduce.output.fileoutputformat.compress.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value><source>programatically</source></property>
<property><name>mapreduce.shuffle.max.connections</name><value>0</value><source>programatically</source></property>
<property><name>dfs.namenode.safemode.extension</name><value>30000</value><source>programatically</source></property>
<property><name>mapreduce.shuffle.port</name><value>13562</value><source>programatically</source></property>
<property><name>eclipse.plug-in.socks.proxy.host</name><value>host</value><source>programatically</source></property>
<property><name>mapreduce.reduce.log.level</name><value>INFO</value><source>programatically</source></property>
<property><name>dfs.datanode.sync.behind.writes</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.instrumentation</name><value>org.apache.hadoop.mapred.JobTrackerMetricsInst</value><source>programatically</source></property>
<property><name>dfs.https.server.keystore.resource</name><value>ssl-server.xml</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.attr.group.name</name><value>cn</value><source>programatically</source></property>
<property><name>dfs.namenode.replication.min</name><value>1</value><source>programatically</source></property>
<property><name>fs.client.resolve.remote.symlinks</name><value>true</value><source>programatically</source></property>
<property><name>s3native.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.tasks.sleeptimebeforesigkill</name><value>5000</value><source>programatically</source></property>
<property><name>tfile.fs.output.buffer.size</name><value>262144</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.persist.jobstatus.active</name><value>true</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.hdfs.impl</name><value>org.apache.hadoop.fs.Hdfs</value><source>programatically</source></property>
<property><name>mapreduce.job.map.output.collector.class</name><value>org.apache.hadoop.mapred.MapTask$MapOutputBuffer</value><source>programatically</source></property>
<property><name>dfs.namenode.safemode.min.datanodes</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.local.dir.minspacestart</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.security.uid.cache.secs</name><value>14400</value><source>programatically</source></property>
<property><name>dfs.client.write.exclude.nodes.cache.expiry.interval.millis</name><value>600000</value><source>programatically</source></property>
<property><name>dfs.client.https.need-auth</name><value>false</value><source>programatically</source></property>
<property><name>dfs.client.https.keystore.resource</name><value>ssl-client.xml</value><source>programatically</source></property>
<property><name>dfs.namenode.max.objects</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.ssl.client.conf</name><value>ssl-client.xml</value><source>programatically</source></property>
<property><name>dfs.namenode.safemode.threshold-pct</name><value>0.999f</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.local.dir.minspacekill</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.retiredjobs.cache.size</name><value>1000</value><source>programatically</source></property>
<property><name>dfs.blocksize</name><value>134217728</value><source>programatically</source></property>
<property><name>mapreduce.job.reduce.slowstart.completedmaps</name><value>0.05</value><source>programatically</source></property>
<property><name>mapreduce.job.end-notification.retry.attempts</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.outofband.heartbeat</name><value>false</value><source>programatically</source></property>
<property><name>io.native.lib.available</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.persist.jobstatus.hours</name><value>1</value><source>programatically</source></property>
<property><name>dfs.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>dfs.namenode.name.dir</name><value>file://${hadoop.tmp.dir}/dfs/name</value><source>programatically</source></property>
<property><name>mapreduce.client.progressmonitor.pollinterval</name><value>1000</value><source>programatically</source></property>
<property><name>dfs.ha.log-roll.period</name><value>120</value><source>programatically</source></property>
<property><name>mapreduce.reduce.input.buffer.percent</name><value>0.0</value><source>programatically</source></property>
<property><name>mapreduce.map.output.compress.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value><source>programatically</source></property>
<property><name>mapreduce.map.skip.proc.count.autoincr</name><value>true</value><source>programatically</source></property>
<property><name>dfs.client.failover.sleep.base.millis</name><value>500</value><source>programatically</source></property>
<property><name>dfs.datanode.directoryscan.threads</name><value>1</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.address</name><value>local</value><source>programatically</source></property>
<property><name>mapreduce.cluster.local.dir</name><value>${hadoop.tmp.dir}/mapred/local</value><source>programatically</source></property>
<property><name>dfs.permissions.enabled</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.taskcontroller</name><value>org.apache.hadoop.mapred.DefaultTaskController</value><source>programatically</source></property>
<property><name>dfs.support.append</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.parallelcopies</name><value>5</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.heartbeats.in.second</name><value>100</value><source>programatically</source></property>
<property><name>mapreduce.job.maxtaskfailures.per.tracker</name><value>3</value><source>programatically</source></property>
<property><name>ipc.client.connection.maxidletime</name><value>10000</value><source>programatically</source></property>
<property><name>mapreduce.shuffle.ssl.enabled</name><value>false</value><source>programatically</source></property>
<property><name>dfs.namenode.invalidate.work.pct.per.iteration</name><value>0.32f</value><source>programatically</source></property>
<property><name>dfs.blockreport.intervalMsec</name><value>21600000</value><source>programatically</source></property>
<property><name>fs.s3.sleepTimeSeconds</name><value>10</value><source>programatically</source></property>
<property><name>dfs.namenode.replication.considerLoad</name><value>true</value><source>programatically</source></property>
<property><name>dfs.client.block.write.retries</name><value>3</value><source>programatically</source></property>
<property><name>hadoop.ssl.server.conf</name><value>ssl-server.xml</value><source>programatically</source></property>
<property><name>dfs.namenode.name.dir.restore</name><value>false</value><source>programatically</source></property>
<property><name>dfs.datanode.hdfs-blocks-metadata.enabled</name><value>false</value><source>programatically</source></property>
<property><name>ha.zookeeper.parent-znode</name><value>/hadoop-ha</value><source>programatically</source></property>
<property><name>io.seqfile.lazydecompress</name><value>true</value><source>programatically</source></property>
<property><name>dfs.https.enable</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.reduce.merge.inmem.threshold</name><value>1000</value><source>programatically</source></property>
<property><name>mapreduce.input.fileinputformat.split.minsize</name><value>0</value><source>programatically</source></property>
<property><name>dfs.replication</name><value>3</value><source>programatically</source></property>
<property><name>ipc.client.tcpnodelay</name><value>false</value><source>programatically</source></property>
<property><name>eclipse.plug-in.user.name</name><value>apple</value><source>programatically</source></property>
<property><name>dfs.namenode.accesstime.precision</name><value>3600000</value><source>programatically</source></property>
<property><name>s3.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.tasktracker.maxblacklists</name><value>4</value><source>programatically</source></property>
<property><name>nfs3.server.port</name><value>2049</value><source>programatically</source></property>
<property><name>mapreduce.job.jvm.numtasks</name><value>1</value><source>programatically</source></property>
<property><name>mapreduce.task.io.sort.mb</name><value>100</value><source>programatically</source></property>
<property><name>io.file.buffer.size</name><value>4096</value><source>programatically</source></property>
<property><name>mapreduce.job.tracker</name><value>localhost:50020</value><source>programatically</source></property>
<property><name>dfs.namenode.audit.loggers</name><value>default</value><source>programatically</source></property>
<property><name>dfs.namenode.checkpoint.txns</name><value>1000000</value><source>programatically</source></property>
<property><name>mapreduce.job.split.metainfo.maxsize</name><value>10000000</value><source>programatically</source></property>
<property><name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB</name><value>org.apache.hadoop.ipc.ProtobufRpcEngine</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms</name><value>1000</value><source>programatically</source></property>
<property><name>mapreduce.reduce.maxattempts</name><value>4</value><source>programatically</source></property>
<property><name>dfs.ha.tail-edits.period</name><value>60</value><source>programatically</source></property>
<property><name>hadoop.security.authentication</name><value>simple</value><source>programatically</source></property>
<property><name>fs.s3.buffer.dir</name><value>${hadoop.tmp.dir}/s3</value><source>programatically</source></property>
<property><name>eclipse.plug-in.socks.proxy.enable</name><value>no</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.taskscheduler</name><value>org.apache.hadoop.mapred.JobQueueTaskScheduler</value><source>programatically</source></property>
<property><name>dfs.namenode.avoid.read.stale.datanode</name><value>false</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.job.task.listener.thread-count</name><value>30</value><source>programatically</source></property>
<property><name>mapreduce.job.reduces</name><value>1</value><source>programatically</source></property>
<property><name>mapreduce.map.sort.spill.percent</name><value>0.80</value><source>programatically</source></property>
<property><name>eclipse.plug-in.namenode.host</name><value>localhost</value><source>programatically</source></property>
<property><name>dfs.client.file-block-storage-locations.timeout</name><value>60</value><source>programatically</source></property>
<property><name>dfs.datanode.drop.cache.behind.writes</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.job.end-notification.retry.interval</name><value>1000</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.minicluster.fixed.ports</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.job.maps</name><value>2</value><source>programatically</source></property>
<property><name>mapreduce.job.speculative.slownodethreshold</name><value>1.0</value><source>programatically</source></property>
<property><name>dfs.block.access.token.enable</name><value>false</value><source>programatically</source></property>
<property><name>tfile.fs.input.buffer.size</name><value>262144</value><source>programatically</source></property>
<property><name>mapreduce.map.speculative</name><value>true</value><source>programatically</source></property>
<property><name>dfs.journalnode.http-address</name><value>0.0.0.0:8480</value><source>programatically</source></property>
<property><name>mapreduce.job.acl-view-job</name><value> </value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.retry-delay.max.ms</name><value>60000</value><source>programatically</source></property>
<property><name>mapreduce.job.end-notification.max.retry.interval</name><value>5000</value><source>programatically</source></property>
<property><name>ftp.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.http.threads</name><value>40</value><source>programatically</source></property>
<property><name>dfs.datanode.data.dir</name><value>file://${hadoop.tmp.dir}/dfs/data</value><source>programatically</source></property>
<property><name>ha.failover-controller.cli-check.rpc-timeout.ms</name><value>20000</value><source>programatically</source></property>
<property><name>dfs.namenode.max.extra.edits.segments.retained</name><value>10000</value><source>programatically</source></property>
<property><name>mapreduce.job.token.tracking.ids.enabled</name><value>false</value><source>programatically</source></property>
<property><name>dfs.namenode.replication.interval</name><value>3</value><source>programatically</source></property>
<property><name>mapreduce.task.skip.start.attempts</name><value>2</value><source>programatically</source></property>
<property><name>dfs.namenode.https-address</name><value>0.0.0.0:50470</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.persist.jobstatus.dir</name><value>/jobtracker/jobsInfo</value><source>programatically</source></property>
<property><name>hadoop.socks.server</name><value>host:1080</value><source>programatically</source></property>
<property><name>dfs.ha.automatic-failover.enabled</name><value>false</value><source>programatically</source></property>
<property><name>ipc.client.kill.max</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.keytab</name><value>/etc/security/keytab/jhs.service.keytab</value><source>programatically</source></property>
<property><name>dfs.image.transfer.timeout</name><value>600000</value><source>programatically</source></property>
<property><name>dfs.client.failover.sleep.max.millis</name><value>15000</value><source>programatically</source></property>
<property><name>mapreduce.job.end-notification.max.attempts</name><value>5</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.max-age-ms</name><value>604800000</value><source>programatically</source></property>
<property><name>mapreduce.task.tmp.dir</name><value>./tmp</value><source>programatically</source></property>
<property><name>dfs.default.chunk.view.size</name><value>32768</value><source>programatically</source></property>
<property><name>hadoop.http.filter.initializers</name><value>org.apache.hadoop.http.lib.StaticUserWebFilter</value><source>programatically</source></property>
<property><name>eclipse.plug-in.socks.proxy.port</name><value>1080</value><source>programatically</source></property>
<property><name>dfs.datanode.failed.volumes.tolerated</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.type</name><value>simple</value><source>programatically</source></property>
<property><name>dfs.umaskmode</name><value>022</value><source>because fs.permissions.umask-mode is deprecated</source></property>
<property><name>dfs.datanode.data.dir.perm</name><value>700</value><source>programatically</source></property>
<property><name>ipc.server.listen.queue.size</name><value>128</value><source>programatically</source></property>
<property><name>mapreduce.reduce.skip.maxgroups</name><value>0</value><source>programatically</source></property>
<property><name>file.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>dfs.namenode.fs-limits.max-directory-items</name><value>0</value><source>programatically</source></property>
<property><name>io.mapfile.bloom.size</name><value>1048576</value><source>programatically</source></property>
<property><name>mapreduce.map.maxattempts</name><value>4</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.jobhistory.block.size</name><value>3145728</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.job.committer.cancel-timeout</name><value>60000</value><source>programatically</source></property>
<property><name>ftp.replication</name><value>3</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.http.address</name><value>0.0.0.0:50030</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.intermediate-done-dir</name><value>${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.address</name><value>0.0.0.0:10020</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.taskcache.levels</name><value>2</value><source>programatically</source></property>
<property><name>dfs.datanode.dns.nameserver</name><value>default</value><source>programatically</source></property>
<property><name>mapred.child.java.opts</name><value>-Xmx200m</value><source>programatically</source></property>
<property><name>dfs.replication.max</name><value>512</value><source>programatically</source></property>
<property><name>map.sort.class</name><value>org.apache.hadoop.util.QuickSort</value><source>programatically</source></property>
<property><name>dfs.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>dfs.namenode.backup.address</name><value>0.0.0.0:50100</value><source>programatically</source></property>
<property><name>hadoop.util.hash.type</name><value>murmur</value><source>programatically</source></property>
<property><name>topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value><source>because net.topology.node.switch.mapping.impl is deprecated</source></property>
<property><name>dfs.block.access.key.update.interval</name><value>600</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.move.interval-ms</name><value>180000</value><source>programatically</source></property>
<property><name>dfs.datanode.use.datanode.hostname</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.reduce.skip.proc.count.autoincr</name><value>true</value><source>programatically</source></property>
<property><name>dfs.datanode.dns.interface</name><value>default</value><source>programatically</source></property>
<property><name>dfs.namenode.backup.http-address</name><value>0.0.0.0:50105</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.http.policy</name><value>HTTP_ONLY</value><source>programatically</source></property>
<property><name>ha.zookeeper.acl</name><value>world:anyone:rwcda</value><source>programatically</source></property>
<property><name>dfs.namenode.enable.retrycache</name><value>true</value><source>programatically</source></property>
<property><name>io.map.index.skip</name><value>0</value><source>programatically</source></property>
<property><name>net.topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value><source>programatically</source></property>
<property><name>fs.s3.maxRetries</name><value>4</value><source>programatically</source></property>
<property><name>dfs.namenode.logging.level</name><value>info</value><source>programatically</source></property>
<property><name>ha.failover-controller.new-active.rpc-timeout.ms</name><value>60000</value><source>programatically</source></property>
<property><name>s3native.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>hadoop.http.staticuser.user</name><value>dr.who</value><source>programatically</source></property>
<property><name>mapreduce.reduce.speculative</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.client.output.filter</name><value>FAILED</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.datestring.cache.size</name><value>200000</value><source>programatically</source></property>
<property><name>mapreduce.ifile.readahead.bytes</name><value>4194304</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.report.address</name><value>127.0.0.1:0</value><source>programatically</source></property>
<property><name>mapreduce.task.userlog.limit.kb</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.map.tasks.maximum</name><value>2</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.simple.anonymous.allowed</name><value>true</value><source>programatically</source></property>
<property><name>hadoop.fuse.timer.period</name><value>5</value><source>programatically</source></property>
<property><name>dfs.namenode.num.extra.edits.retained</name><value>1000000</value><source>programatically</source></property>
<property><name>mapreduce.job.classloader.system.classes</name><value>java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.</value><source>programatically</source></property>
<property><name>hadoop.rpc.socket.factory.class.default</name><value>org.apache.hadoop.net.StandardSocketFactory</value><source>programatically</source></property>
<property><name>dfs.namenode.handler.count</name><value>10</value><source>programatically</source></property>
<property><name>fs.automatic.close</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.healthchecker.script.timeout</name><value>600000</value><source>programatically</source></property>
<property><name>dfs.datanode.directoryscan.interval</name><value>21600</value><source>programatically</source></property>
<property><name>yarn.resourcemanager.address</name><value>0.0.0.0:8032</value><source>programatically</source></property>
<property><name>dfs.client.file-block-storage-locations.num-threads</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.reduce.markreset.buffer.percent</name><value>0.0</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.directory.search.timeout</name><value>10000</value><source>programatically</source></property>
<property><name>mapreduce.map.log.level</name><value>INFO</value><source>programatically</source></property>
<property><name>dfs.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>dfs.namenode.checkpoint.max-retries</name><value>3</value><source>programatically</source></property>
<property><name>dfs.namenode.avoid.write.stale.datanode</name><value>false</value><source>programatically</source></property>
<property><name>ftp.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>ha.health-monitor.rpc-timeout.ms</name><value>45000</value><source>programatically</source></property>
<property><name>dfs.namenode.edits.noeditlogchannelflush</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.attr.member</name><value>member</value><source>programatically</source></property>
<property><name>dfs.blockreport.initialDelay</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.job.classloader</name><value>false</value><source>programatically</source></property>
<property><name>io.compression.codec.bzip2.library</name><value>system-native</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.token.validity</name><value>36000</value><source>programatically</source></property>
<property><name>dfs.namenode.delegation.token.max-lifetime</name><value>604800000</value><source>programatically</source></property>
<property><name>s3native.replication</name><value>3</value><source>programatically</source></property>
<property><name>dfs.heartbeat.interval</name><value>3</value><source>programatically</source></property>
<property><name>dfs.ha.fencing.ssh.connect-timeout</name><value>30000</value><source>programatically</source></property>
<property><name>net.topology.impl</name><value>org.apache.hadoop.net.NetworkTopology</value><source>programatically</source></property>
<property><name>mapreduce.task.profile</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.instrumentation</name><value>org.apache.hadoop.mapred.TaskTrackerMetricsInst</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.http.address</name><value>0.0.0.0:50060</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.webapp.address</name><value>0.0.0.0:19888</value><source>programatically</source></property>
<property><name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name><value>5000</value><source>programatically</source></property>
<property><name>eclipse.plug-in.jobtracker.host</name><value>localhost</value><source>programatically</source></property>
<property><name>mapreduce.job.ubertask.maxmaps</name><value>9</value><source>programatically</source></property>
<property><name>eclipse.plug-in.namenode.port</name><value>9000</value><source>programatically</source></property>
<property><name>mapreduce.job.userlog.retain.hours</name><value>24</value><source>programatically</source></property>
<property><name>dfs.namenode.secondary.http-address</name><value>0.0.0.0:50090</value><source>programatically</source></property>
<property><name>mapreduce.task.timeout</name><value>600000</value><source>programatically</source></property>
<property><name>mapreduce.framework.name</name><value>local</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.loadedjobs.cache.size</name><value>5</value><source>programatically</source></property>
<property><name>ipc.client.idlethreshold</name><value>4000</value><source>programatically</source></property>
<property><name>ipc.server.tcpnodelay</name><value>false</value><source>programatically</source></property>
<property><name>ftp.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>dfs.namenode.stale.datanode.interval</name><value>30000</value><source>programatically</source></property>
<property><name>s3.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>mapreduce.job.speculative.slowtaskthreshold</name><value>1.0</value><source>programatically</source></property>
<property><name>fs.s3.block.size</name><value>67108864</value><source>programatically</source></property>
<property><name>dfs.client.failover.connection.retries</name><value>0</value><source>programatically</source></property>
<property><name>mapreduce.job.queuename</name><value>default</value><source>programatically</source></property>
<property><name>hadoop.rpc.protection</name><value>authentication</value><source>programatically</source></property>
<property><name>dfs.namenode.retrycache.expirytime.millis</name><value>600000</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.client-am.ipc.max-retries</name><value>3</value><source>programatically</source></property>
<property><name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name><value>${dfs.web.authentication.kerberos.principal}</value><source>programatically</source></property>
<property><name>ftp.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>fs.defaultFS</name><value>hdfs://localhost:9000/</value><source>because fs.default.name is deprecated</source></property>
<property><name>mapreduce.task.merge.progress.records</name><value>10000</value><source>programatically</source></property>
<property><name>file.client-write-packet-size</name><value>65536</value><source>programatically</source></property>
<property><name>mapreduce.reduce.cpu.vcores</name><value>1</value><source>programatically</source></property>
<property><name>fs.trash.checkpoint.interval</name><value>0</value><source>programatically</source></property>
<property><name>hadoop.http.authentication.signature.secret.file</name><value>${user.home}/hadoop-http-auth-signature-secret</value><source>programatically</source></property>
<property><name>dfs.df.interval</name><value>60000</value><source>because fs.df.interval is deprecated</source></property>
<property><name>s3native.stream-buffer-size</name><value>4096</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.read.timeout</name><value>180000</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.command-opts</name><value>-Xmx1024m</value><source>programatically</source></property>
<property><name>mapreduce.admin.user.env</name><value>LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native</value><source>programatically</source></property>
<property><name>dfs.namenode.checkpoint.edits.dir</name><value>${dfs.namenode.checkpoint.dir}</value><source>programatically</source></property>
<property><name>mapreduce.local.clientfactory.class.name</name><value>org.apache.hadoop.mapred.LocalClientFactory</value><source>programatically</source></property>
<property><name>fs.permissions.umask-mode</name><value>022</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.move.thread-count</name><value>3</value><source>programatically</source></property>
<property><name>hadoop.common.configuration.version</name><value>0.23.0</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.dns.interface</name><value>default</value><source>programatically</source></property>
<property><name>mapreduce.output.fileoutputformat.compress.type</name><value>RECORD</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.ssl</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.ifile.readahead</name><value>true</value><source>programatically</source></property>
<property><name>io.serializations</name><value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</value><source>programatically</source></property>
<property><name>fs.df.interval</name><value>60000</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.input.buffer.percent</name><value>0.70</value><source>programatically</source></property>
<property><name>io.seqfile.compress.blocksize</name><value>1000000</value><source>programatically</source></property>
<property><name>ipc.client.connect.max.retries</name><value>10</value><source>programatically</source></property>
<property><name>hadoop.security.groups.cache.secs</name><value>300</value><source>programatically</source></property>
<property><name>dfs.namenode.delegation.key.update-interval</name><value>86400000</value><source>programatically</source></property>
<property><name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name><value>0.75f</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.client.max-retries</name><value>3</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping.ldap.search.filter.user</name><value>(&amp;(objectClass=user)(sAMAccountName={0}))</value><source>programatically</source></property>
<property><name>dfs.image.compress</name><value>false</value><source>programatically</source></property>
<property><name>dfs.namenode.kerberos.internal.spnego.principal</name><value>${dfs.web.authentication.kerberos.principal}</value><source>programatically</source></property>
<property><name>fs.s3n.block.size</name><value>67108864</value><source>programatically</source></property>
<property><name>fs.ftp.host</name><value>0.0.0.0</value><source>programatically</source></property>
<property><name>hadoop.security.group.mapping</name><value>org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.resource.cpu-vcores</name><value>1</value><source>programatically</source></property>
<property><name>dfs.datanode.address</name><value>0.0.0.0:50010</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.cleaner.enable</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.map.skip.maxrecords</name><value>0</value><source>programatically</source></property>
<property><name>dfs.datanode.https.address</name><value>0.0.0.0:50475</value><source>programatically</source></property>
<property><name>file.replication</name><value>1</value><source>programatically</source></property>
<property><name>dfs.datanode.drop.cache.behind.reads</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.fuse.connection.timeout</name><value>300</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.restart.recover</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.work.around.non.threadsafe.getpwuid</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.indexcache.mb</name><value>10</value><source>programatically</source></property>
<property><name>mapreduce.output.fileoutputformat.compress</name><value>false</value><source>programatically</source></property>
<property><name>hadoop.tmp.dir</name><value>/tmp/hadoop-${user.name}</value><source>programatically</source></property>
<property><name>dfs.client.block.write.replace-datanode-on-failure.policy</name><value>DEFAULT</value><source>programatically</source></property>
<property><name>hadoop.kerberos.kinit.command</name><value>kinit</value><source>programatically</source></property>
<property><name>mapreduce.job.committer.setup.cleanup.needed</name><value>true</value><source>programatically</source></property>
<property><name>topology.script.number.args</name><value>100</value><source>because net.topology.script.number.args is deprecated</source></property>
<property><name>dfs.webhdfs.enabled</name><value>false</value><source>programatically</source></property>
<property><name>dfs.namenode.fs-limits.max-blocks-per-file</name><value>1048576</value><source>programatically</source></property>
<property><name>dfs.datanode.du.reserved</name><value>0</value><source>programatically</source></property>
<property><name>fs.default.name</name><value>hdfs://localhost:9000/</value><source>programatically</source></property>
<property><name>mapreduce.task.profile.reduces</name><value>0-2</value><source>programatically</source></property>
<property><name>file.bytes-per-checksum</name><value>512</value><source>programatically</source></property>
<property><name>dfs.client.block.write.replace-datanode-on-failure.enable</name><value>true</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.handler.count</name><value>10</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.job.committer.commit-window</name><value>10000</value><source>programatically</source></property>
<property><name>net.topology.script.number.args</name><value>100</value><source>programatically</source></property>
<property><name>mapreduce.task.profile.maps</name><value>0-2</value><source>programatically</source></property>
<property><name>dfs.namenode.decommission.interval</name><value>30</value><source>programatically</source></property>
<property><name>dfs.image.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.system.dir</name><value>${hadoop.tmp.dir}/mapred/system</value><source>programatically</source></property>
<property><name>dfs.namenode.support.allow.format</name><value>true</value><source>programatically</source></property>
<property><name>hadoop.ssl.hostname.verifier</name><value>DEFAULT</value><source>programatically</source></property>
<property><name>ipc.client.connect.timeout</name><value>20000</value><source>programatically</source></property>
<property><name>io.mapfile.bloom.error.rate</name><value>0.005</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.principal</name><value>jhs/_HOST@REALM.TLD</value><source>programatically</source></property>
<property><name>dfs.permissions.superusergroup</name><value>supergroup</value><source>programatically</source></property>
<property><name>mapreduce.shuffle.ssl.file.buffer.size</name><value>65536</value><source>programatically</source></property>
<property><name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name><value>10737418240</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.expire.trackers.interval</name><value>600000</value><source>programatically</source></property>
<property><name>mapreduce.cluster.acls.enabled</name><value>false</value><source>programatically</source></property>
<property><name>ha.failover-controller.graceful-fence.connection.retries</name><value>1</value><source>programatically</source></property>
<property><name>ha.health-monitor.connect-retry-interval.ms</name><value>1000</value><source>programatically</source></property>
<property><name>dfs.namenode.checkpoint.check.period</name><value>60</value><source>programatically</source></property>
<property><name>io.seqfile.local.dir</name><value>${hadoop.tmp.dir}/io/local</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.resource.mb</name><value>1536</value><source>programatically</source></property>
<property><name>mapreduce.reduce.shuffle.merge.percent</name><value>0.66</value><source>programatically</source></property>
<property><name>tfile.io.chunk.size</name><value>1048576</value><source>programatically</source></property>
<property><name>file.blocksize</name><value>67108864</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.jobhistory.lru.cache.size</name><value>5</value><source>programatically</source></property>
<property><name>mapreduce.jobtracker.maxtasks.perjob</name><value>-1</value><source>programatically</source></property>
<property><name>mapreduce.job.acl-modify-job</name><value> </value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.reduce.tasks.maximum</name><value>2</value><source>programatically</source></property>
<property><name>mapreduce.am.max-attempts</name><value>2</value><source>programatically</source></property>
<property><name>mapreduce.cluster.temp.dir</name><value>${hadoop.tmp.dir}/mapred/temp</value><source>programatically</source></property>
<property><name>io.skip.checksum.errors</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.jobhistory.joblist.cache.size</name><value>20000</value><source>programatically</source></property>
<property><name>yarn.app.mapreduce.am.staging-dir</name><value>/tmp/hadoop-yarn/staging</value><source>programatically</source></property>
<property><name>dfs.namenode.edits.journal-plugin.qjournal</name><value>org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager</value><source>programatically</source></property>
<property><name>dfs.datanode.handler.count</name><value>10</value><source>programatically</source></property>
<property><name>dfs.namenode.decommission.nodes.per.interval</name><value>5</value><source>programatically</source></property>
<property><name>fs.ftp.host.port</name><value>21</value><source>programatically</source></property>
<property><name>dfs.namenode.checkpoint.period</name><value>3600</value><source>programatically</source></property>
<property><name>dfs.namenode.fs-limits.max-component-length</name><value>0</value><source>programatically</source></property>
<property><name>fs.AbstractFileSystem.viewfs.impl</name><value>org.apache.hadoop.fs.viewfs.ViewFs</value><source>programatically</source></property>
<property><name>dfs.namenode.retrycache.heap.percent</name><value>0.03f</value><source>programatically</source></property>
<property><name>mapreduce.tasktracker.dns.nameserver</name><value>default</value><source>programatically</source></property>
<property><name>hadoop.native.lib</name><value>true</value><source>because io.native.lib.available is deprecated</source></property>
<property><name>ipc.client.fallback-to-simple-auth-allowed</name><value>false</value><source>programatically</source></property>
<property><name>mapreduce.map.output.compress</name><value>false</value><source>programatically</source></property>
<property><name>dfs.datanode.ipc.address</name><value>0.0.0.0:50020</value><source>programatically</source></property>
<property><name>hadoop.ssl.require.client.cert</name><value>false</value><source>programatically</source></property>
<property><name>dfs.datanode.max.transfer.threads</name><value>4096</value><source>programatically</source></property>
</configuration>